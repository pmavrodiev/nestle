{
  "paragraphs": [
    {
      "text": "%spark\n\nimport org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n/* spark context is in \u0027sc \u0027 */\n\n/* Create a Hive context for interaction with Hive via HiveQL */\n//var hiveContext \u003d new org.apache.spark.sql.hive.HiveContext(sc)\n\nvar sparkConf \u003d sc.getConf.getAll\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 8:44:00 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478688594580_-949696631",
      "id": "20161109-114954_1956231180",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport org.apache.spark._\n\nimport org.apache.spark.graphx._\n\nimport org.apache.spark.rdd.RDD\nsparkConf: Array[(String, String)] \u003d Array((spark.history.kerberos.keytab,none), (spark.eventLog.enabled,true), (master,yarn-client), (zeppelin.spark.concurrentSQL,false), (spark.yarn.containerLauncherMaxThreads,25), (spark.driver.extraLibraryPath,/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64), (spark.driver.host,10.38.228.200), (spark.executorEnv.PYTHONPATH,/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip:/usr/hdp/current/spark-client/python/:/usr/hdp/current/spark-client/python:/usr/hdp/current/spark-client/python/lib/py4j-0.8.2.1-src.zip\u003cCPS\u003e{{PWD}}/pyspark.zip\u003cCPS\u003e{{PWD}}/py4j-0.9-src.zip), (spark.executor.extraLibraryPath,/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64),..."
      },
      "dateCreated": "Nov 9, 2016 11:49:54 AM",
      "dateStarted": "Nov 10, 2016 8:44:00 AM",
      "dateFinished": "Nov 10, 2016 8:44:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "dateUpdated": "Nov 10, 2016 3:33:29 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478788409568_-536431297",
      "id": "20161110-153329_1884965847",
      "dateCreated": "Nov 10, 2016 3:33:29 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define vertex types",
      "text": "%spark\n\n/* Define the different types of vertices */\nclass VertexProperty(val propertyName: String, val id: Int) extends Serializable // base class\ncase class ReadNode(val name: String, val read_id: Int) extends VertexProperty(name, read_id) \ncase class ContigNode(val name: String, val contig_id: Int, val organism_id: Int) extends VertexProperty(name, contig_id) \ncase class OrganismNode(val name: String, val organism_id: Int) extends VertexProperty(name, organism_id) \n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 11:30:55 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478688611909_862116588",
      "id": "20161109-115011_894699029",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ndefined class VertexProperty\n\ndefined class ReadNode\n\ndefined class ContigNode\n\ndefined class OrganismNode\n"
      },
      "dateCreated": "Nov 9, 2016 11:50:11 AM",
      "dateStarted": "Nov 10, 2016 11:30:55 AM",
      "dateFinished": "Nov 10, 2016 11:30:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define edge types",
      "text": "%spark\n\nclass EdgeBase(val _type: String) extends Serializable // base calss\n\n/*** read-to-contig edge between a ReadNode and a ContigNode ***/\n/*\n    pos:\n        The start position at which the read was matched to the contig\n    count:\n        The number of characters matched counting from the starting position\n    quality:\n        The quality of the match. DEFAULT \u003d 0\n    conf:\n        The confidence of the match. To be computed after the whole graph is contructed. \n        Every edge is created with default confidence of 1.0. DEFAULT \u003d 1.0\n*/\n\ncase class RCedge(pos: Int, count: Int \u003d 20, \n                  quality: Double \u003d 0.0, conf: Double \u003d 1.0) extends EdgeBase(\"RCedge\")\n\n/*** read-to-read edge between 2 nodes of type ReadNode ***/\n/*\n    contig_node_id: \n        The node_id of the contig to which the 2 nodes (reads) are mapped\n    weight: \n        The closeness between the 2 nodes.  Can be quantified as a function of the positions to which the nodes map onto\n        the contig. For example: 1 / abs(pos2 - pos1)\n*/\ncase class RRedge(contig_node_id: Long, weight: Double) extends EdgeBase(\"RRedge\")\n\n/*** contig-to-organism edge between a contig and an organism***/\n/* the mapping between contig and organism must be injective */\ncase class COedge(edge_id: Int) extends EdgeBase(\"COedge\")",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 12:06:17 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478699382624_-535018651",
      "id": "20161109-144942_1588913755",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ndefined class EdgeBase\n\ndefined class RCedge\n\ndefined class RRedge\n\ndefined class COedge\n"
      },
      "dateCreated": "Nov 9, 2016 2:49:42 PM",
      "dateStarted": "Nov 10, 2016 12:06:17 PM",
      "dateFinished": "Nov 10, 2016 12:06:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize the nodes",
      "text": "%spark\nnodes match {\n    case _: RDD[_] \u003d\u003e nodes.unpersist() \n}\n\nval nodes: RDD[(Long, VertexProperty)] \u003d \n    sc.parallelize(Array((0L,  ReadNode(name \u003d \"read\", read_id \u003d 1)),\n                         (1L,  ReadNode(name \u003d \"read\", read_id \u003d 2)),\n                         (2L,  ReadNode(name \u003d \"read\", read_id \u003d 3)),\n                         (3L,  ReadNode(name \u003d \"read\", read_id \u003d 4)),\n                         (4L,  ReadNode(name \u003d \"read\", read_id \u003d 5)),\n                         (5L,  ReadNode(name \u003d \"read\", read_id \u003d 6)),\n                         (6L,  ReadNode(name \u003d \"read\", read_id \u003d 7)),\n                         (7L,  ContigNode(name \u003d \"contig\", contig_id \u003d 1, organism_id \u003d 1)),\n                         (8L,  ContigNode(name \u003d \"contig\", contig_id \u003d 2, organism_id \u003d 2)),\n                         (9L,  ContigNode(name \u003d \"contig\", contig_id \u003d 3, organism_id \u003d 2)),\n                         (10L, ContigNode(name \u003d \"contig\", contig_id \u003d 4, organism_id \u003d 3)),\n                         (11L, OrganismNode(name \u003d \"organism\", organism_id \u003d 1)),\n                         (12L, OrganismNode(name \u003d \"organism\", organism_id \u003d 2)),\n                         (13L, OrganismNode(name \u003d \"organism\", organism_id \u003d 3))\n    ))",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 12:06:18 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478690821287_1809356868",
      "id": "20161109-122701_929294798",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres215: nodes.type \u003d ParallelCollectionRDD[156] at parallelize at \u003cconsole\u003e:58\n\nnodes: org.apache.spark.rdd.RDD[(Long, VertexProperty)] \u003d ParallelCollectionRDD[158] at parallelize at \u003cconsole\u003e:58\n"
      },
      "dateCreated": "Nov 9, 2016 12:27:01 PM",
      "dateStarted": "Nov 10, 2016 12:06:18 PM",
      "dateFinished": "Nov 10, 2016 12:06:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Initialize the edges",
      "text": "%spark\nedges match {\n    case _: RDD[_] \u003d\u003e edges.unpersist() \n}\n\nval edges: RDD[Edge[EdgeBase]]\u003d \n    sc.parallelize(Array(\n            /* contig-to-organism (CO) edges */\n             Edge(7L, 11L, COedge(edge_id \u003d 0).asInstanceOf[EdgeBase]),\n             Edge(8L, 12L, COedge(edge_id \u003d 1).asInstanceOf[EdgeBase]),\n             Edge(9L, 12L, COedge(edge_id \u003d 2).asInstanceOf[EdgeBase]),\n             Edge(10L, 13L,COedge(edge_id \u003d 3).asInstanceOf[EdgeBase]),\n            /* read-to-contig (RC) edges */\n             Edge(0L, 8L,  RCedge(pos \u003d 0).asInstanceOf[EdgeBase]),\n             Edge(1L, 8L,  RCedge(pos \u003d 1).asInstanceOf[EdgeBase]),\n             Edge(2L, 8L,  RCedge(pos \u003d 2).asInstanceOf[EdgeBase]),\n             Edge(2L, 7L,  RCedge(pos \u003d 2).asInstanceOf[EdgeBase]),\n             Edge(2L, 10L, RCedge(pos \u003d 2).asInstanceOf[EdgeBase]),\n             Edge(3L, 8L,  RCedge(pos \u003d 3).asInstanceOf[EdgeBase]),\n             Edge(3L, 9L,  RCedge(pos \u003d 3).asInstanceOf[EdgeBase]),\n             Edge(3L, 10L, RCedge(pos \u003d 4).asInstanceOf[EdgeBase]),\n             Edge(4L, 8L,  RCedge(pos \u003d 4).asInstanceOf[EdgeBase]),\n             Edge(5L, 8L,  RCedge(pos \u003d 5).asInstanceOf[EdgeBase]),\n             Edge(6L, 8L,  RCedge(pos \u003d 6).asInstanceOf[EdgeBase])\n    ))",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:19 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478701018132_-1305140929",
      "id": "20161109-151658_1077425242",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres360: edges.type \u003d ParallelCollectionRDD[159] at parallelize at \u003cconsole\u003e:56\n\nedges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[EdgeBase]] \u003d ParallelCollectionRDD[378] at parallelize at \u003cconsole\u003e:56\n"
      },
      "dateCreated": "Nov 9, 2016 3:16:58 PM",
      "dateStarted": "Nov 10, 2016 3:28:19 PM",
      "dateFinished": "Nov 10, 2016 3:28:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create RRedges from existing RCedges (helper function)",
      "text": "%spark\n\n/* Calculate similarity between 2 Reads that have been mapped to the same Contig\n   In principle, this similarity, or weight, can be a function of multiple factors,\n   e.g., the difference in the positions the reads have been mapped to, the quality/length of the match, etc.\n   \n   Here, we use only the absolute difference in positions.\n   */\ndef calculateWeight(edge1: Edge[EdgeBase], edge2: Edge[EdgeBase]): Double \u003d {\n    val pos_1 \u003d edge1.attr.asInstanceOf[RCedge].pos\n    val pos_2 \u003d edge2.attr.asInstanceOf[RCedge].pos\n    Math.abs(pos_1 - pos_2)\n}\n\n// create a paired RDD\n//(Edge(0,8,RCedge(0,20,0.0,1.0)),Edge(0,8,RCedge(0,20,0.0,1.0)))\ndef constructRRedge(edge1: Edge[EdgeBase], edge2: Edge[EdgeBase]) \u003d {\n    val src_1 \u003d edge1.srcId\n    val src_2 \u003d edge2.srcId\n    val dest_1 \u003d edge1.dstId\n    val dest_2 \u003d edge2.dstId\n    if ( dest_1 \u003d\u003d dest_2 \u0026\u0026 src_1 !\u003d src_2) {\n        // the two Reads have been mapped to the same Contig -\u003e create RRedge\n        val weight \u003d calculateWeight(edge1, edge2)\n        // this returns a key-\u003evalue pair\n        (if (src_1 \u003c src_2) (src_1, src_2) else (src_2, src_1), // simple key\n            Edge(src_1, src_2, // the value is an Edge of type RRedge\n             RRedge(contig_node_id \u003d dest_1, weight \u003d weight).asInstanceOf[EdgeBase]))\n    }\n    else null\n}\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:22 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478716015377_-890466900",
      "id": "20161109-192655_51737905",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ncalculateWeight: (edge1: org.apache.spark.graphx.Edge[EdgeBase], edge2: org.apache.spark.graphx.Edge[EdgeBase])Double\n\nconstructRRedge: (edge1: org.apache.spark.graphx.Edge[EdgeBase], edge2: org.apache.spark.graphx.Edge[EdgeBase])((org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId), org.apache.spark.graphx.Edge[EdgeBase])\n"
      },
      "dateCreated": "Nov 9, 2016 7:26:55 PM",
      "dateStarted": "Nov 10, 2016 3:28:22 PM",
      "dateFinished": "Nov 10, 2016 3:28:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create RRedges from existing RCedges",
      "text": "%spark\n\nRC_edges match {\n    case _: RDD[_] \u003d\u003e RC_edges.unpersist()\n}\n\n\nval RC_edges \u003d edges\n                .filter(e \u003d\u003e (e.attr).isInstanceOf[RCedge])\n\nval RR_edges \u003d RC_edges\n                .cartesian(RC_edges) // (Edge(0,8,RCedge(0,20,0.0,1.0)),Edge(1,8,RCedge(1,20,0.0,1.0)))\n                .map(x \u003d\u003e constructRRedge(x._1,  // Edge(0,8,RCedge(0,20,0.0,1.0))\n                                          x._2)) // Edge(1,8,RCedge(1,20,0.0,1.0))\n                .filter(x \u003d\u003e x !\u003d null) //remove everything that is not an RRedge\n                //.reduceByKey( (x,y) \u003d\u003e \n                //        if (x.srcId \u003c y.srcId) x\n                //        else y) //((2,6),Edge(*6*,2,RRedge(8,4.0))) \u003d\u003e ((2,6),Edge(*2*,6,RRedge(8,4.0)))\n                .map(x \u003d\u003e x._2) //drop the keys, get only the values, i.e. the RR edges\n                \n                \n                \n                \nRR_edges.collect().foreach(println(_))                \n                \n                \n                \n                \n                \n                \n                \n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:24 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 195.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478710911923_-1540614800",
      "id": "20161109-180151_2009481780",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres362: RC_edges.type \u003d MapPartitionsRDD[373] at filter at \u003cconsole\u003e:58\n\nRC_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[EdgeBase]] \u003d MapPartitionsRDD[379] at filter at \u003cconsole\u003e:58\n\nRR_edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[EdgeBase]] \u003d MapPartitionsRDD[383] at map at \u003cconsole\u003e:78\nEdge(0,1,RRedge(8,1.0))\nEdge(0,2,RRedge(8,2.0))\nEdge(1,0,RRedge(8,1.0))\nEdge(1,2,RRedge(8,1.0))\nEdge(2,0,RRedge(8,2.0))\nEdge(2,1,RRedge(8,1.0))\nEdge(0,3,RRedge(8,3.0))\nEdge(0,4,RRedge(8,4.0))\nEdge(0,5,RRedge(8,5.0))\nEdge(0,6,RRedge(8,6.0))\nEdge(1,3,RRedge(8,2.0))\nEdge(1,4,RRedge(8,3.0))\nEdge(1,5,RRedge(8,4.0))\nEdge(1,6,RRedge(8,5.0))\nEdge(2,3,RRedge(8,1.0))\nEdge(2,4,RRedge(8,2.0))\nEdge(2,5,RRedge(8,3.0))\nEdge(2,6,RRedge(8,4.0))\nEdge(3,0,RRedge(8,3.0))\nEdge(3,1,RRedge(8,2.0))\nEdge(3,2,RRedge(8,1.0))\nEdge(4,0,RRedge(8,4.0))\nEdge(4,1,RRedge(8,3.0))\nEdge(4,2,RRedge(8,2.0))\nEdge(5,0,RRedge(8,5.0))\nEdge(5,1,RRedge(8,4.0))\nEdge(5,2,RRedge(8,3.0))\nEdge(6,0,RRedge(8,6.0))\nEdge(6,1,RRedge(8,5.0))\nEdge(6,2,RRedge(8,4.0))\nEdge(2,3,RRedge(10,2.0))\nEdge(3,4,RRedge(8,1.0))\nEdge(3,5,RRedge(8,2.0))\nEdge(3,6,RRedge(8,3.0))\nEdge(3,2,RRedge(10,2.0))\nEdge(4,3,RRedge(8,1.0))\nEdge(4,5,RRedge(8,1.0))\nEdge(4,6,RRedge(8,2.0))\nEdge(5,3,RRedge(8,2.0))\nEdge(5,4,RRedge(8,1.0))\nEdge(5,6,RRedge(8,1.0))\nEdge(6,3,RRedge(8,3.0))\nEdge(6,4,RRedge(8,2.0))\nEdge(6,5,RRedge(8,1.0))\n"
      },
      "dateCreated": "Nov 9, 2016 6:01:51 PM",
      "dateStarted": "Nov 10, 2016 3:28:24 PM",
      "dateFinished": "Nov 10, 2016 3:28:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Update the graph edges and initialize the actual graph",
      "text": "%spark\ngraph match {\n    case _: Graph[_,_] \u003d\u003e graph.unpersist()\n}\n\n// first add the RR edges to the edge list of the graph \nval graph \u003d Graph(nodes, edges.union(RR_edges))\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:33 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478778954629_1742750694",
      "id": "20161110-125554_1082661944",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres364: org.apache.spark.graphx.Graph[VertexProperty,EdgeBase] \u003d org.apache.spark.graphx.impl.GraphImpl@4c8293bb\n\ngraph: org.apache.spark.graphx.Graph[VertexProperty,EdgeBase] \u003d org.apache.spark.graphx.impl.GraphImpl@7f2593ce\n"
      },
      "dateCreated": "Nov 10, 2016 12:55:54 PM",
      "dateStarted": "Nov 10, 2016 3:28:33 PM",
      "dateFinished": "Nov 10, 2016 3:28:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define custom degree metric",
      "text": "%spark\n\ncase class CustomDegree(val contig_id: Int, val contig_node_id:Int, val degree: Double) extends Serializable\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:37 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478781524506_1944638766",
      "id": "20161110-133844_1794146834",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ndefined class CustomDegree\n"
      },
      "dateCreated": "Nov 10, 2016 1:38:44 PM",
      "dateStarted": "Nov 10, 2016 3:28:37 PM",
      "dateFinished": "Nov 10, 2016 3:28:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\n// the map is contig_id:Long -\u003e  weighted degree:Double\ncase class CustomDegree(val degree2contig : collection.mutable.Map[Long, Double]) extends Serializable\n    \nval weighted_degrees: VertexRDD[CustomDegree] \u003d graph.aggregateMessages[CustomDegree] (\n    triplet \u003d\u003e { // map function, triplet represents an EdgeContext[VD, ED, Msg], triplet.attr gives the edge attribute, i.e. EdgeBase\n        // srcAttr \u003d (Long, VertexProperty)\n        if (triplet.attr.isInstanceOf[RRedge]) {\n            // create message: send the id of the contig node, together with the weight of the RR link\n            val theEdge \u003d triplet.attr.asInstanceOf[RRedge]\n            val msg \u003d collection.mutable.Map[Long, Double](theEdge.contig_node_id -\u003e theEdge.weight)\n            triplet.sendToDst(CustomDegree(msg))\n        }\n    },\n    (msg1, msg2) \u003d\u003e { //reduce function, takes 2 CustomDegree messages\n        val map1 : collection.mutable.Map[Long,Double] \u003d msg1.degree2contig\n        val map2 : collection.mutable.Map[Long,Double]\u003d msg2.degree2contig\n        map1.foreach( kv \u003d\u003e  map2(kv._1) \u003d map2.getOrElseUpdate(kv._1, 0) + kv._2)\n        CustomDegree(map2)\n    },\n    TripletFields.EdgeOnly // pass only the edge attributes to the EdgeContext in the mapper\n)\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478707599713_-1598711515",
      "id": "20161109-170639_163072442",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ndefined class CustomDegree\n\nweighted_degrees: org.apache.spark.graphx.VertexRDD[CustomDegree] \u003d VertexRDDImpl[400] at RDD at VertexRDD.scala:57\n"
      },
      "dateCreated": "Nov 9, 2016 5:06:39 PM",
      "dateStarted": "Nov 10, 2016 3:28:39 PM",
      "dateFinished": "Nov 10, 2016 3:28:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nweighted_degrees.collect().foreach(println(_))",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 3:28:42 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478785437357_945822922",
      "id": "20161110-144357_995099272",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "(4,CustomDegree(Map(8 -\u003e 13.0)))\n(0,CustomDegree(Map(8 -\u003e 21.0)))\n(6,CustomDegree(Map(8 -\u003e 21.0)))\n(2,CustomDegree(Map(8 -\u003e 13.0, 10 -\u003e 2.0)))\n(1,CustomDegree(Map(8 -\u003e 16.0)))\n(3,CustomDegree(Map(8 -\u003e 12.0, 10 -\u003e 2.0)))\n(5,CustomDegree(Map(8 -\u003e 16.0)))\n"
      },
      "dateCreated": "Nov 10, 2016 2:43:57 PM",
      "dateStarted": "Nov 10, 2016 3:28:42 PM",
      "dateFinished": "Nov 10, 2016 3:28:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nval m \u003d collection.mutable.Map[String, Int](\"a\" -\u003e 1)\nval m1 \u003d collection.mutable.Map[String, Int](\"a\" -\u003e 3, \"b\" -\u003e 1 )",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 2:43:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478765116449_281218115",
      "id": "20161110-090516_281113335",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nm: scala.collection.mutable.Map[String,Int] \u003d Map(a -\u003e 1)\n\nm1: scala.collection.mutable.Map[String,Int] \u003d Map(b -\u003e 1, a -\u003e 3)\n"
      },
      "dateCreated": "Nov 10, 2016 9:05:16 AM",
      "dateStarted": "Nov 10, 2016 2:43:15 PM",
      "dateFinished": "Nov 10, 2016 2:43:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval m3 \u003d m.foreach( kv \u003d\u003e  m1(kv._1) \u003d m1.getOrElseUpdate(kv._1, 0) + kv._2)\n",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 2:43:17 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478782732426_209135524",
      "id": "20161110-135852_1347289262",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nm3: Unit \u003d ()\n"
      },
      "dateCreated": "Nov 10, 2016 1:58:52 PM",
      "dateStarted": "Nov 10, 2016 2:43:17 PM",
      "dateFinished": "Nov 10, 2016 2:43:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nm3",
      "user": "admin",
      "dateUpdated": "Nov 10, 2016 2:43:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478782765641_-895231737",
      "id": "20161110-135925_2000274964",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Nov 10, 2016 1:59:25 PM",
      "dateStarted": "Nov 10, 2016 2:43:20 PM",
      "dateFinished": "Nov 10, 2016 2:43:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "dateUpdated": "Nov 10, 2016 2:01:28 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1478782888401_1645032099",
      "id": "20161110-140128_1011453779",
      "dateCreated": "Nov 10, 2016 2:01:28 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "pavlin-Spark",
  "id": "2C3DAA6KY",
  "angularObjects": {
    "2C2FVSTPQ:shared_process": [],
    "2C28DETC2:shared_process": [],
    "2C24RZY6N:shared_process": [],
    "2BZ5WHQHY:shared_process": [],
    "2C2XP7J9D:shared_process": [],
    "2C1TJ69QM:shared_process": [],
    "2BZCDVXQG:shared_process": [],
    "2C16E3V6Y:shared_process": [],
    "2C31ZM3HN:shared_process": [],
    "2BYP3Q9VB:shared_process": [],
    "2BZRZ4XD8:shared_process": [],
    "2C28U48M2:shared_process": [],
    "2BZ76V9TV:shared_process": [],
    "2C2YR1C92:shared_process": [],
    "2C3BAXZXW:shared_process": [],
    "2C2DMU7N5:shared_process": [],
    "2C2ZY9GQ2:shared_process": [],
    "2BZ896HDX:shared_process": [],
    "2BZZY31R4:shared_process": []
  },
  "config": {},
  "info": {}
}